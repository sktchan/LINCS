{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7cedc9",
   "metadata": {},
   "source": [
    "<< to keep track of changes made, prev. ideas, etc. >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f3a3fe",
   "metadata": {},
   "source": [
    "apr 8, 2025 - flux_transf.jl\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b69bd4",
   "metadata": {},
   "source": [
    "this is what was used for original 2D input into mhsa (which doesn't work, needs (k, q, v) input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff8976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function (tf::Transf)(input::Float32Matrix2DType) # input is features (978) x batch\n",
    "    sa_out = tf.mhsa(tf.norm_1(input)) # OG paper states norm after sa, but norm before sa is more common?\n",
    "    # x = input + tf.dropout(sa_out)\n",
    "    x = input + sa_out\n",
    "    mlp_out = tf.mlp(tf.norm_2(x))\n",
    "    # x = x + tf.dropout(mlp_out)\n",
    "    x = x + mlp_out\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c617cb3",
   "metadata": {},
   "source": [
    "apr 11, 2025 - flux_transf.jl\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc0b2b",
   "metadata": {},
   "source": [
    "using a for loop and replacing a matrix of -1 rather than making a copy is much faster! \n",
    "using the function below (commented parts) is a little easier to read, but the current impl. in the file is much more efficient.\n",
    "tmp: if we were to replace the -1s with not the gene names (Str) but with the ranking ints themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9089216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sort_gene(expr)\n",
    "    # data_ranked = Matrix{Int}(undef, size(expr))\n",
    "    data_ranked = fill(-1, size(expr))\n",
    "    # gs = Symbol.(gene_symbols)\n",
    "    n, m = size(expr)\n",
    "    p = Vector{Int}(undef, n)\n",
    "    # tmp = sortperm(expr[:, 1])\n",
    "\n",
    "    for j in 1:m\n",
    "        e = view(expr, :, j)\n",
    "        sortperm!(p, e, rev=true)\n",
    "        # data_ranked[!, j] = gs[tmp]\n",
    "\n",
    "        for i in 1:n\n",
    "            data_ranked[i, j] = p[i]\n",
    "        end\n",
    "    end\n",
    "    return data_ranked\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec85ec5f",
   "metadata": {},
   "source": [
    "GENES AS TOKENS gene-gene interactions (sequence length as len(genes))\n",
    "- each gene is a position in your sequence\n",
    "- each token's embedding contains information about that gene's ranking across samples\n",
    "- the sequence length equals the number of genes you're considering\n",
    "SAMPLES AS TOKENS sample-sample interactions (sequence length as len(samples))\n",
    "- each sample is a position in your sequence\n",
    "- each token's embedding contains the gene ranking information for that sample\n",
    "- the sequence length equals the number of samples\n",
    "TECHNICAL CONSIDERATIONS\n",
    "transformers struggle with very long sequences, so if we have many more genes than samples, using samples as tokens may be more computationally feasible\n",
    "self-attention complexity grows quadratically with sequence length\n",
    "which dimension has more examples to learn from?\n",
    "\n",
    "good option: Flux.MultiHeadAttention((64, 64, 64) => (64, 64) => 64, nheads=1), can incr nheads later\n",
    "- q, k, v input dim should all be the same if data type is the same or we aren't doing encoder-decoder\n",
    "- middle dimensions should also be the same unless we want to reduce computational complexity in the middle\n",
    "- output can also be the same unless we want to do ft compression or expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8df65",
   "metadata": {},
   "source": [
    "may 1, 2025 - masked loss fxn - DONE\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec7019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_masked(model, x, y_masked)\n",
    "    logits = model(x)  # (n_classes, seq_len, batch_size)\n",
    "    logits = permutedims(logits, (2, 3, 1))  # seq_len × batch_size × n_classes (to match targets)\n",
    "    logits = reshape(logits, :, n_classes)   # (seq_len * batch_size) × n_classes\n",
    "\n",
    "    y_masked_flat = vec(y_masked) # flatten\n",
    "    # only keep where y_masked != -100\n",
    "    mask = y_masked_flat .!= -100\n",
    "    logits_masked = (logits[mask, :])'\n",
    "    targets_masked = y_masked_flat[mask]\n",
    "    y_oh = Flux.onehotbatch(targets_masked, 1:n_classes)\n",
    "\n",
    "    return Flux.logitcrossentropy(logits_masked, y_oh)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f723c",
   "metadata": {},
   "source": [
    "may 27, 2025 - training fxn with masked values for accuracy - DONE\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ffb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss(model, x, y)\n",
    "    logits = model(x)  # (n_classes, seq_len, batch_size)\n",
    "    logits_flat = reshape(logits, size(logits, 1), :) # (n_classes, seq_len*batch_size)\n",
    "    y_flat = vec(y) # (seq_len*batch_size) column vec\n",
    "\n",
    "    mask = y_flat .!= -100 # bit vec, where sum = n_masked\n",
    "    logits_masked = logits_flat[:, mask] # (n_classes, n_masked)\n",
    "    y_masked = y_flat[mask] # (n_masked) column vec\n",
    "\n",
    "    y_oh = Flux.onehotbatch(y_masked, 1:n_classes) # (n_classes, n_masked)\n",
    "    return Flux.logitcrossentropy(logits_masked, y_oh) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505e3725",
   "metadata": {},
   "source": [
    "could return logits_masked and y_masked as well, then do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_masked = Flux.onecold(logits_masked)\n",
    "preds_masked_cpu = preds_masked |> cpu\n",
    "preds_masked_cpu .== y_masked\n",
    "accuracy = sum(preds_masked_cpu .== y_masked) / length(y_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604ef7db",
   "metadata": {},
   "source": [
    "may 29, 2025 - sparse matrices\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in train loop, instead of y_gpu:\n",
    "y_batch_sparse = get_sparse_batch(y_train_masked, start_idx, end_idx)\n",
    "\n",
    "using SparseArrays\n",
    "\n",
    "function mask_input_sparse(X::Matrix{Int64}; mask_ratio=0.10)\n",
    "    X_masked = copy(X)\n",
    "    # Create sparse matrix for labels\n",
    "    I_indices = Int[]  # row indices\n",
    "    J_indices = Int[]  # column indices  \n",
    "    values = Int16[]   # actual gene indices\n",
    "    \n",
    "    for j in 1:size(X, 2)\n",
    "        num_masked = ceil(Int, size(X, 1) * mask_ratio)\n",
    "        mask_positions = randperm(size(X, 1))[1:num_masked]\n",
    "        \n",
    "        for pos in mask_positions\n",
    "            push!(I_indices, pos)\n",
    "            push!(J_indices, j)\n",
    "            push!(values, X[pos, j])  # original gene index\n",
    "            \n",
    "            X_masked[pos, j] = MASK_ID\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    y_sparse = sparse(I_indices, J_indices, values, size(X)...)\n",
    "    return X_masked, y_sparse\n",
    "end\n",
    "\n",
    "X_train_masked, y_train_masked = mask_input_sparse(X_train)\n",
    "X_test_masked, y_test_masked = mask_input_sparse(X_test)\n",
    "\n",
    "function loss_sparse(model, x, y_sparse_batch, mode)\n",
    "    logits = model(x)  # (n_classes, seq_len, batch_size)\n",
    "    \n",
    "    rows_cpu, cols_cpu, vals_cpu = findnz(y_sparse_batch)\n",
    "    \n",
    "    if isempty(rows_cpu)\n",
    "        return 0.0f0\n",
    "    end\n",
    "    \n",
    "    rows_gpu = cu(rows_cpu)\n",
    "    cols_gpu = cu(cols_cpu) \n",
    "    vals_gpu = cu(vals_cpu)\n",
    "    \n",
    "    batch_size = size(logits, 3)\n",
    "    seq_len = size(logits, 2)\n",
    "    \n",
    "    linear_indices = (cols_gpu .- 1) .* seq_len .+ rows_gpu\n",
    "    \n",
    "    logits_reshaped = reshape(logits, size(logits, 1), :) # (n_classes, seq_len * batch_size)\n",
    "    masked_logits = logits_reshaped[:, linear_indices]  # (n_classes, n_masked)\n",
    "    \n",
    "    y_oh = Flux.onehotbatch(vals_gpu, 1:n_classes)\n",
    "    \n",
    "    if mode == \"train\"\n",
    "        return Flux.logitcrossentropy(masked_logits, y_oh)\n",
    "    elseif mode == \"test\"\n",
    "        return Flux.logitcrossentropy(masked_logits, y_oh), masked_logits, vals_gpu\n",
    "    end\n",
    "end\n",
    "\n",
    "function get_sparse_batch(y_sparse, start_idx, end_idx)\n",
    "    rows, cols, vals = findnz(y_sparse[:, start_idx:end_idx])\n",
    "    cols_adjusted = cols\n",
    "    batch_sparse = sparse(rows, cols_adjusted, vals, size(y_sparse, 1), end_idx - start_idx + 1)\n",
    "    \n",
    "    return batch_sparse\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e28e3c",
   "metadata": {},
   "source": [
    "** with the above code, 11mins 1 epoch (see github @ this time for other params) VS. 11mins 1 epoch for dense representations.\n",
    "can revisit later if there is found to be memory bottlenecks @ matrix operations, however for now the dense is sufficient b/c:\n",
    "https://www.reddit.com/r/Julia/comments/108g5ou/when_is_it_worth_working_with_sparse_matrices/\n",
    "https://medium.com/data-science/sparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71\n",
    "- above state that there should be about 1% or less sparsity for GPU sparse matrices to be efficient\n",
    "- due to the need to repeatedly transfer data between CPU and GPU and sparse slicing operations in batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29a6d02",
   "metadata": {},
   "source": [
    "june 12, 2025 trying dynamic masking;\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981827d4",
   "metadata": {},
   "source": [
    "**RoBERTa shows that masking a different subset every epoch already helps, \n",
    "but recent work finds that decreasing the rate during training is even better \n",
    "(to try later!!! aka scheduler)\n",
    "\n",
    "***dynamic on the train, static on the test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6fe77",
   "metadata": {},
   "source": [
    "jun 16, 2025 - tried to only mask 1 position; if it can't predict just the missing # from 1-978, then it's dumb.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb01a0e7",
   "metadata": {},
   "source": [
    "- the issue here might be that the test mask is different from the train mask; \n",
    "thus w/o dynamic masking, the train learns a single value each time, and the test provides a new value not seen before..?\n",
    "\n",
    "also trying to profile the memory b/c 25min per epoch is way too long;\n",
    "Profile.Allocs.@profile sample_rate=1 begin/end is taking wayyyyy too long too (~3hrs so far) to run in the REPL.. maybe\n",
    "is there another way?\n",
    "\n",
    "*also what takes 26h on kraken takes 4h on smaug...\n",
    "there seems to be slightly better trainval loss using higher embed dim --> try higher dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5c7136",
   "metadata": {},
   "source": [
    "jun 20, 2025 - based on the results from smaug, 2025-06-19_21-48\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107b8d3",
   "metadata": {},
   "source": [
    "there seems to be an issue with learning diff masked tokens (1 per sample) across the whole dataset\n",
    "it works if we want to do the same say, 5 masks across the whole dataset (albeit at only a 84% accuracy for some reason)\n",
    "1. double check masking function - ensure that it is correct\n",
    "2. scale up learning rate..? not sure what else to do here\n",
    "the training masks have sufficinet examples to learn from i think (60 per label) so what is going wrong?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1b0b2",
   "metadata": {},
   "source": [
    "jul 24, 2025 - speed/memory optimization\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac89a91",
   "metadata": {},
   "source": [
    "- lux.jl is equivalent of jax in python (uses xla backend)\n",
    "- unrelated, but wb a splitted data struct..? (what lea uses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d657f",
   "metadata": {},
   "source": [
    "jul 25, 2025 - exp masking...?\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d397d7bf",
   "metadata": {},
   "source": [
    "1. can you embed counts? how would that work\n",
    "2. needs to be a regression output (1 value) rather than a vector of probabilities per class?\n",
    "3. loss needs to be defined differently\n",
    "4. should it just be a dense network? does MHA work on "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae0d249",
   "metadata": {},
   "source": [
    "jul 30, 2025 - exp masking\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a574d2d",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Dense layer instead of Flux.Embedding \n",
    "    - this is b/c below static vs. dynamic Embeddings\n",
    "    - dense layer stores f(x) = Wx + b, where W and b are learned during training for all genes/samples\n",
    "    - W is a matrix, b is a vector and these are multiplied by the input x to output f(x) the embeddign vector \n",
    "- MHA still works with non-tokenized values!\n",
    "    - model learns the meaning of the expression levels rather than the gene identity/ranks?\n",
    "- mask token should be 0.0 after normalizing input\n",
    "- MSE loss\n",
    "- regression output to 1 value\n",
    "\n",
    "Static Embeddings (like Word2Vec or Flux.Embedding): \n",
    "- The model learns one single, fixed vector for each unique token (e.g., the word \"bank\"). \n",
    "- The goal is to learn the meaning of the token itself by averaging its usage across thousands of different contexts.\n",
    "Dynamic Feature Representation (Your Model): \n",
    "- Your model does not learn a static vector for \"Gene 1\". \n",
    "- Instead, it learns a function that maps any given expression value to a vector representation.\n",
    "\n",
    "9PM - edit to exp masking\n",
    "- had to move loss calc to before model update - resulted in lower loss in test than train\n",
    "- change masking val to -1, apparenlty there are exp elvels of 0 in the original dataset?\n",
    "\n",
    "***shoudl make a plot similar to this scatter plot for check_error.jl file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93ac6e4",
   "metadata": {},
   "source": [
    "aug 1, 2025 - pre lab-meeting\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5948cb",
   "metadata": {},
   "source": [
    "***need to fix the logging of params for predstrues.csv and params.txt ; didn't save in the last couple runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbd354f",
   "metadata": {},
   "source": [
    "aug 4, 2025 - debug + seb loss issue\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0740cc47",
   "metadata": {},
   "source": [
    "- runnign on GPU 2 for indef run - rerun for seb\n",
    "- need to \n",
    "    1. figure out logging for test/train - is it diff than what's in the slides?\n",
    "    2. debug original structure typing\n",
    "    3. fix logging of params for predstrues.csv and params.txt when doing input comparison plots\n",
    "    4. fix progressbar for indef_run.jl (why is it out of 628, repeats for each epoch???)\n",
    "\n",
    "- 08-04 run is original test/loss definitions from creating 720ep graph\n",
    "- changed code to use mask_transf code in the while loop\n",
    "- so Flux.withgradient = 1st loss calc --> update! --> second loss calc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b97cb",
   "metadata": {},
   "source": [
    "aug 5, 2025 - debug\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b30855",
   "metadata": {},
   "source": [
    "original structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85463f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct PosEnc\n",
    "    pe_matrix::CuArray{Float32,2}\n",
    "end\n",
    "\n",
    "function PosEnc(embed_dim::Int, max_len::Int) # max_len is usually maximum length of sequence but here it is just len(genes)\n",
    "    pe_matrix = Matrix{Float32}(undef, embed_dim, max_len)\n",
    "    for pos in 1:max_len, i in 1:embed_dim\n",
    "        angle = pos / (10000^(2*(div(i-1,2))/embed_dim))\n",
    "        if mod(i, 2) == 1\n",
    "            pe_matrix[i,pos] = sin(angle) # odd indices\n",
    "        else\n",
    "            pe_matrix[i,pos] = cos(angle) # even indices\n",
    "        end\n",
    "    end\n",
    "    return PosEnc(cu(pe_matrix))\n",
    "end\n",
    "\n",
    "Flux.@functor PosEnc\n",
    "\n",
    "function (pe::PosEnc)(input::Float32Matrix3DType)\n",
    "    seq_len = size(input,2)\n",
    "    return input .+ pe.pe_matrix[:,1:seq_len] # adds positional encoding to input embeddings\n",
    "end\n",
    "\n",
    "### building transformer section\n",
    "\n",
    "struct Transf\n",
    "    mha::Flux.MultiHeadAttention\n",
    "    att_dropout::Flux.Dropout\n",
    "    att_norm::Flux.LayerNorm # this is the normalization aspect\n",
    "    mlp::Flux.Chain\n",
    "    mlp_norm::Flux.LayerNorm\n",
    "end\n",
    "\n",
    "function Transf(\n",
    "    embed_dim::Int, \n",
    "    hidden_dim::Int; \n",
    "    n_heads::Int, \n",
    "    dropout_prob::Float64\n",
    "    )\n",
    "\n",
    "    mha = Flux.MultiHeadAttention((embed_dim, embed_dim, embed_dim) => (embed_dim, embed_dim) => embed_dim, \n",
    "                                    nheads=n_heads, \n",
    "                                    dropout_prob=dropout_prob\n",
    "                                    )\n",
    "\n",
    "    att_dropout = Flux.Dropout(dropout_prob)\n",
    "    \n",
    "    att_norm = Flux.LayerNorm(embed_dim)\n",
    "    \n",
    "    mlp = Flux.Chain(\n",
    "        Flux.Dense(embed_dim => hidden_dim, gelu),\n",
    "        Flux.Dropout(dropout_prob),\n",
    "        Flux.Dense(hidden_dim => embed_dim),\n",
    "        Flux.Dropout(dropout_prob)\n",
    "        )\n",
    "    mlp_norm = Flux.LayerNorm(embed_dim)\n",
    "\n",
    "    return Transf(mha, att_dropout, att_norm, mlp, mlp_norm)\n",
    "end\n",
    "\n",
    "Flux.@functor Transf\n",
    "\n",
    "function (tf::Transf)(input::Float32Matrix3DType) # input shape: embed_dim × seq_len × batch_size\n",
    "    normed = tf.att_norm(input)\n",
    "    atted = tf.mha(normed, normed, normed)[1] # outputs a tuple (a, b)\n",
    "    att_dropped = tf.att_dropout(atted)\n",
    "    residualed = input + att_dropped\n",
    "    res_normed = tf.mlp_norm(residualed)\n",
    "\n",
    "    embed_dim, seq_len, batch_size = size(res_normed)\n",
    "    reshaped = reshape(res_normed, embed_dim, seq_len * batch_size) # dense layers expect 2D inputs\n",
    "    mlp_out = tf.mlp(reshaped)\n",
    "    mlp_out_reshaped = reshape(mlp_out, embed_dim, seq_len, batch_size)\n",
    "    \n",
    "    tf_output = residualed + mlp_out_reshaped\n",
    "    return tf_output\n",
    "end\n",
    "\n",
    "### full model as << ranked data --> token embedding --> position embedding --> transformer --> classifier head >>\n",
    "\n",
    "struct Model\n",
    "    embedding::Flux.Embedding\n",
    "    pos_encoder::PosEnc\n",
    "    pos_dropout::Flux.Dropout\n",
    "    transformer::Flux.Chain\n",
    "    classifier::Flux.Chain\n",
    "end\n",
    "\n",
    "function Model(;\n",
    "    input_size::Int,\n",
    "    embed_dim::Int,\n",
    "    n_layers::Int,\n",
    "    n_classes::Int,\n",
    "    n_heads::Int,\n",
    "    hidden_dim::Int,\n",
    "    dropout_prob::Float64\n",
    "    )\n",
    "\n",
    "    embedding = Flux.Embedding(input_size => embed_dim)\n",
    "\n",
    "    pos_encoder = PosEnc(embed_dim, input_size)\n",
    "\n",
    "    pos_dropout = Flux.Dropout(dropout_prob)\n",
    "\n",
    "    transformer = Flux.Chain(\n",
    "        [Transf(embed_dim, hidden_dim; n_heads, dropout_prob) for _ in 1:n_layers]...\n",
    "        )\n",
    "\n",
    "    classifier = Flux.Chain(\n",
    "        Flux.Dense(embed_dim => embed_dim, gelu),\n",
    "        Flux.LayerNorm(embed_dim),\n",
    "        Flux.Dense(embed_dim => n_classes)\n",
    "        )\n",
    "\n",
    "    return Model(embedding, pos_encoder, pos_dropout, transformer, classifier)\n",
    "end\n",
    "\n",
    "Flux.@functor Model\n",
    "\n",
    "function (model::Model)(input::IntMatrix2DType)\n",
    "    embedded = model.embedding(input)\n",
    "    encoded = model.pos_encoder(embedded)\n",
    "    encoded_dropped = model.pos_dropout(encoded)\n",
    "    transformed = model.transformer(encoded_dropped)\n",
    "    # pooled = dropdims(mean(transformed; dims=2), dims=2)\n",
    "    logits_output = model.classifier(transformed)\n",
    "    return logits_output\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ae10f",
   "metadata": {},
   "source": [
    "re-typed structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaccda7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct PosEnc{U<:AbstractMatrix}\n",
    "    pe_matrix::U\n",
    "end\n",
    "\n",
    "function PosEnc(embed_dim::Int, max_len::Int) # max_len is usually maximum length of sequence but here it is just len(genes)\n",
    "    pe_matrix = Matrix{Float32}(undef, embed_dim, max_len)\n",
    "    for pos in 1:max_len, i in 1:embed_dim\n",
    "        angle = pos / (10000^(2*(div(i-1,2))/embed_dim))\n",
    "        if mod(i, 2) == 1\n",
    "            pe_matrix[i,pos] = sin(angle) # odd indices\n",
    "        else\n",
    "            pe_matrix[i,pos] = cos(angle) # even indices\n",
    "        end\n",
    "    end\n",
    "    return PosEnc(pe_matrix)\n",
    "end\n",
    "\n",
    "Flux.@functor PosEnc\n",
    "\n",
    "function (pe::PosEnc)(input::Float32Matrix3DType)\n",
    "    seq_len = size(input,2)\n",
    "    return input .+ pe.pe_matrix[:,1:seq_len] # adds positional encoding to input embeddings\n",
    "end\n",
    "\n",
    "### building transformer section\n",
    "\n",
    "struct Transf{MHA<:Flux.MultiHeadAttention, D<:Flux.Dropout, LN<:Flux.LayerNorm, C<:Flux.Chain}\n",
    "    mha::MHA\n",
    "    att_dropout::D\n",
    "    att_norm::LN\n",
    "    mlp::C\n",
    "    mlp_norm::LN\n",
    "end\n",
    "\n",
    "function Transf(\n",
    "    embed_dim::Int, \n",
    "    hidden_dim::Int; \n",
    "    n_heads::Int, \n",
    "    dropout_prob::Float64\n",
    "    )\n",
    "\n",
    "    mha = Flux.MultiHeadAttention((embed_dim, embed_dim, embed_dim) => (embed_dim, embed_dim) => embed_dim, \n",
    "                                    nheads=n_heads, \n",
    "                                    dropout_prob=dropout_prob\n",
    "                                    )\n",
    "\n",
    "    att_dropout = Flux.Dropout(dropout_prob)\n",
    "    \n",
    "    att_norm = Flux.LayerNorm(embed_dim)\n",
    "    \n",
    "    mlp = Flux.Chain(\n",
    "        Flux.Dense(embed_dim => hidden_dim, gelu),\n",
    "        Flux.Dropout(dropout_prob),\n",
    "        Flux.Dense(hidden_dim => embed_dim),\n",
    "        Flux.Dropout(dropout_prob)\n",
    "        )\n",
    "    mlp_norm = Flux.LayerNorm(embed_dim)\n",
    "\n",
    "    return Transf(mha, att_dropout, att_norm, mlp, mlp_norm)\n",
    "end\n",
    "\n",
    "Flux.@functor Transf\n",
    "\n",
    "function (tf::Transf)(input::Float32Matrix3DType) # input shape: embed_dim × seq_len × batch_size\n",
    "    normed = tf.att_norm(input)\n",
    "    atted, _ = tf.mha(normed, normed, normed) # outputs a tuple (a, b)\n",
    "    att_dropped = tf.att_dropout(atted)\n",
    "    residualed = input + att_dropped\n",
    "    res_normed = tf.mlp_norm(residualed)\n",
    "\n",
    "    embed_dim, seq_len, batch_size = size(res_normed)\n",
    "    reshaped = reshape(res_normed, embed_dim, seq_len * batch_size) # dense layers expect 2D inputs\n",
    "    mlp_out = tf.mlp(reshaped)\n",
    "    mlp_out_reshaped = reshape(mlp_out, embed_dim, seq_len, batch_size)\n",
    "    \n",
    "    tf_output = residualed + mlp_out_reshaped\n",
    "    return tf_output\n",
    "end\n",
    "\n",
    "struct Model{E<:Flux.Embedding, P<:PosEnc, D<:Flux.Dropout, T<:Flux.Chain, C<:Flux.Chain}\n",
    "    embedding::E\n",
    "    pos_encoder::P\n",
    "    pos_dropout::D\n",
    "    transformer::T\n",
    "    classifier::C\n",
    "end\n",
    "\n",
    "function Model(;\n",
    "    input_size::Int,\n",
    "    embed_dim::Int,\n",
    "    n_layers::Int,\n",
    "    n_classes::Int,\n",
    "    n_heads::Int,\n",
    "    hidden_dim::Int,\n",
    "    dropout_prob::Float64\n",
    "    )\n",
    "\n",
    "    embedding = Flux.Embedding(input_size => embed_dim)\n",
    "\n",
    "    pos_encoder = PosEnc(embed_dim, input_size)\n",
    "\n",
    "    pos_dropout = Flux.Dropout(dropout_prob)\n",
    "\n",
    "    transformer = Flux.Chain(\n",
    "    (Transf(embed_dim, hidden_dim; n_heads, dropout_prob) for _ in 1:n_layers)...\n",
    "    )\n",
    "\n",
    "    classifier = Flux.Chain(\n",
    "        Flux.Dense(embed_dim => embed_dim, gelu),\n",
    "        Flux.LayerNorm(embed_dim),\n",
    "        Flux.Dense(embed_dim => n_classes)\n",
    "        )\n",
    "\n",
    "    return Model(embedding, pos_encoder, pos_dropout, transformer, classifier)\n",
    "end\n",
    "\n",
    "Flux.@functor Model\n",
    "\n",
    "function (model::Model)(input::T) where {T<:IntMatrix2DType} \n",
    "    # there is an issue here - where type is Any from the Flux portion\n",
    "    # now - if Flux is causing issues, go into source code and redefine as above:\n",
    "    # (m::Embedding)(x::T) where {T<:AbstractArray} = reshape(m(vec(x)), :, size(x)...), copied from Flux source code\n",
    "    # AND\n",
    "    # input::T where T<:type, allows it to be distinguished as a subtype of the input type\n",
    "    # should theoretically be able to avoid Anys, and be type-stable!\n",
    "    embedded = model.embedding(input)\n",
    "    encoded = model.pos_encoder(embedded)\n",
    "    encoded_dropped = model.pos_dropout(encoded)\n",
    "    transformed = model.transformer(encoded_dropped)\n",
    "    pooled = dropdims(mean(transformed; dims=2), dims=2)\n",
    "    logits_output = model.classifier(pooled)\n",
    "    return logits_output\n",
    "    return embedded\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee13bed",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- need to \n",
    "    1. ~~re-run with fixed typing~~ faster.jl running on kraken gpu 1\n",
    "\n",
    "    2. ~~figure out why test is better than train for loss/accuracy (potentially change in indef_run code or run mask_transf code for x epochs if test is only better in indef_run and not mask_transf)~~ ~~indef_run.jl code changed, running new on smaug gpu 2 ONCE OLD_INDEF_RUN.JL gets to 40!!! --> running new one now! what was the diff bruh~~ doen + clarified fix, see indef_masked_rankings 08-04 vs. 08-05. issue was the withgradient (AGAIN!!!)\n",
    "\n",
    "    3. ~~fix param logging for exp_transf + mask_transf (predstrues.csv, params.txt)~~\n",
    "\n",
    "    4. ~~fix progressbar for indef_run.jl~~ removed progress bar lol\n",
    "\n",
    "    5. ~~fix scatter plot for mask_transf comparison~~ ~~mask_transf_err.jl running on kraken gpu 0~~ done, see masked_rankings/2025-08-05\n",
    "\n",
    "    6. ~~x-bin for exp_transf comparison~~ ~~exp_transf.jl running on smaug gpu 3~~ done, see masked_expression/2025-08-05\n",
    "\n",
    "    7. reorganize exp, mask, indef, faster for tomorrow\n",
    "\n",
    "    8. put fxns/structs into separate src files! more organized.\n",
    "\n",
    "- 08-04 run is original test/loss calculations from creating 720ep graph\n",
    "- 08-05 run is updated calculations from mask_transf code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af32f5",
   "metadata": {},
   "source": [
    "aug 6, 2025 - recap\n",
    "="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ad1644",
   "metadata": {},
   "source": [
    "asap:\n",
    "- ~~exp_transf.jl running on kraken 0 (10ep x-bin)~~ done\n",
    "- ~~mask_transf_err.jl running on smaug 3 (10ep heatmap)~~ done\n",
    "\n",
    "still pending:\n",
    "- faster.jl running on kraken gpu 1 --> old code: 668774 ms, new code:\n",
    "    - terminated - need to fix code\n",
    "- reorganize exp, mask, indef, faster\n",
    "- put fxns/structs into separate src files! more organized.\n",
    "- ~~why not: exp transf run on untrt - kraken 0~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f18c7db",
   "metadata": {},
   "source": [
    "aug 12, 2025 - predicting the average, ensuring no repeats, fixing plots\n",
    "="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5098b",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- ~~redo heatmap/x-bin into boxplots or hex-bin?~~\n",
    "    - exp on kraken 0, rank on smaug 0\n",
    "- ~~see if model is just prev the avg rather than acc learning (raw exp)~~\n",
    "    - in exp code, running 100ep on kraken 0 for comparison\n",
    "- see if possible to ensure model has no repeats (via permutations, inductive bias, pointer networks)\n",
    "- put fxns/structs into separate src files! more organized.\n",
    "- do test iwthout pretrain to see if masking even helps\n",
    "- faster.jl ; compare memory usage still high!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82fa216",
   "metadata": {},
   "source": [
    "aug 14, 2025 - for ensuring no repeats\n",
    "="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b80874",
   "metadata": {},
   "source": [
    "a pointer network is designed to select its output from the elements that are **present in the input sequence**. however, the correct answer (the masked number) is the one element that is explicitly absent from the input.\n",
    "\n",
    "https://kierszbaumsamuel.medium.com/pointer-networks-what-are-they-c3cb68fae076#:~:text=Notice%20how%20they%20are%20placed,-%20output%20dictionary%3A\n",
    "\n",
    "https://arxiv.org/abs/2006.06380"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf26e77",
   "metadata": {},
   "source": [
    "alternatively:\n",
    "\n",
    "can have two sets of inputs:\n",
    "- context: masked sequence [1, 2, ..., MASK, 79, ...].\n",
    "- candidate: complete, unmasked set of all possible tokens [1, 2, ..., 978].\n",
    "\n",
    "where:\n",
    "- encoder processes the context input to understand what's missing.\n",
    "- decoder or attention mechanism then uses this context to point to the correct token within the candidate input.\n",
    "\n",
    "thus, the model isn't pointing to the sequence it was given but to a complete \"dictionary\" of possibilities, using the masked sequence to figure out which item in the dictionary is the right one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
