{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0493a3be",
   "metadata": {},
   "source": [
    "1. train/test loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f0d623",
   "metadata": {},
   "source": [
    "original code => test loss below train loss\n",
    "\n",
    "see 08-04 plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d7c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "while true\n",
    "    global epoch += 1\n",
    "\n",
    "    epoch_train_losses = Float32[]\n",
    "    total_train_tp = 0\n",
    "    total_train_masked = 0\n",
    "\n",
    "    for start_idx in 1:batch_size:size(X_train_masked, 2)\n",
    "        end_idx = min(start_idx + batch_size - 1, size(X_train_masked, 2))\n",
    "        x_batch = gpu(X_train_masked[:, start_idx:end_idx])\n",
    "        y_batch = gpu(y_train_masked[:, start_idx:end_idx])\n",
    "\n",
    "        loss_val, grads = Flux.withgradient(model) do m # here is the difference\n",
    "            loss(m, x_batch, y_batch, \"train\")\n",
    "        end\n",
    "        Flux.update!(opt, model, grads[1])\n",
    "\n",
    "        push!(epoch_train_losses, loss_val)\n",
    "\n",
    "        preds_masked = Flux.onecold(logits_masked) |> cpu\n",
    "        tp = sum(preds_masked .== cpu(y_masked))\n",
    "        total_masked = length(y_masked)\n",
    "        total_train_tp += tp\n",
    "        total_train_masked += total_masked\n",
    "    end\n",
    "\n",
    "    # log metrics\n",
    "    epoch_train_loss = mean(epoch_train_losses)\n",
    "    epoch_train_acc = 0.0\n",
    "    if total_train_masked > 0\n",
    "        epoch_train_acc = total_train_tp / total_train_masked\n",
    "    end\n",
    "    push!(train_losses, epoch_train_loss)\n",
    "    push!(train_accuracies, epoch_train_acc)\n",
    "\n",
    "\n",
    "    ### start test    \n",
    "    epoch_test_losses = Float32[]\n",
    "    total_test_tp = 0\n",
    "    total_test_masked = 0\n",
    "\n",
    "    for start_idx in 1:batch_size:size(X_test_masked, 2)\n",
    "        end_idx = min(start_idx + batch_size - 1, size(X_test_masked, 2))\n",
    "        x_batch = gpu(X_test_masked[:, start_idx:end_idx])\n",
    "        y_batch = gpu(y_test_masked[:, start_idx:end_idx])\n",
    "\n",
    "        test_loss_val, logits_masked, y_masked = loss(model, x_batch, y_batch, \"test\")\n",
    "\n",
    "        push!(epoch_test_losses, test_loss_val)\n",
    "\n",
    "        preds_masked = Flux.onecold(logits_masked) |> cpu\n",
    "        tp = sum(preds_masked .== cpu(y_masked))\n",
    "        total_masked = length(y_masked)\n",
    "        total_test_tp += tp\n",
    "        total_test_masked += total_masked\n",
    "    end\n",
    "\n",
    "    # log metrics\n",
    "    epoch_test_loss = mean(epoch_test_losses)\n",
    "\n",
    "    epoch_test_acc = 0.0\n",
    "    if total_test_masked > 0\n",
    "        epoch_test_acc = total_test_tp / total_test_masked\n",
    "    end\n",
    "\n",
    "    push!(test_losses, epoch_test_loss)\n",
    "    push!(test_accuracies, epoch_test_acc)\n",
    "\n",
    "    if epoch % 10 == 0\n",
    "        run_time = now() - start_time\n",
    "        total_minutes = div(run_time.value, 60000)\n",
    "        run_hours = div(total_minutes, 60)\n",
    "        run_minutes = rem(total_minutes, 60)\n",
    "\n",
    "        timestamp = Dates.format(now(), \"yyyy-mm-dd_HH-MM-SS\")\n",
    "        save_dir = joinpath(\"plots\", \"untrt\", \"indef_masked_rankings\", \"$(timestamp)_epoch_$(epoch)\")\n",
    "        mkpath(save_dir)\n",
    "\n",
    "        epochs_ran = 1:epoch\n",
    "\n",
    "        # loss plot\n",
    "        loss_plot = Plots.plot(epochs_ran, train_losses; label=\"train loss\",\n",
    "             xlabel=\"epoch\", ylabel=\"loss\", title=\"train vs. test loss (epoch $epoch)\", lw=2)\n",
    "        Plots.plot!(loss_plot, epochs_ran, test_losses; label=\"test loss\", lw=2)\n",
    "        savefig(joinpath(save_dir, \"trainval_loss.png\"))\n",
    "\n",
    "        # acc plot\n",
    "        acc_plot = Plots.plot(epochs_ran, train_accuracies; label=\"train accuracy\",\n",
    "            xlabel=\"epoch\", ylabel=\"accuracy\", title=\"train vs. test accuracy (epoch $epoch)\", lw=2, legend=:bottomright)\n",
    "        Plots.plot!(acc_plot, epochs_ran, test_accuracies; label=\"test accuracy\", lw=2)\n",
    "        savefig(joinpath(save_dir, \"accuracy.png\"))\n",
    "\n",
    "        # save acc to csv\n",
    "        df = DataFrame(\n",
    "            epoch = epochs_ran,\n",
    "            train_accuracy = train_accuracies,\n",
    "            test_accuracy = test_accuracies,\n",
    "            train_loss = train_losses,\n",
    "            test_loss = test_losses\n",
    "            )\n",
    "        CSV.write(joinpath(save_dir, \"metrics.csv\"), df)\n",
    "\n",
    "        # log params\n",
    "        params_txt = joinpath(save_dir, \"params.txt\")\n",
    "        open(params_txt, \"w\") do io\n",
    "            println(io, \"EPOCH @ SAVE: $epoch\")\n",
    "            println(io, \"TIMESTAMP: $timestamp\")\n",
    "            println(io, \"--------------------\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f88cc0b",
   "metadata": {},
   "source": [
    "new code => loss logging corrected\n",
    "\n",
    "see 08-05 plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03627457",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "while true\n",
    "    global epoch += 1\n",
    "\n",
    "    ### start train\n",
    "    epoch_train_losses = Float32[]\n",
    "    total_train_tp = 0\n",
    "    total_train_masked = 0\n",
    "\n",
    "    for start_idx in 1:batch_size:size(X_train_masked, 2)\n",
    "        end_idx = min(start_idx + batch_size - 1, size(X_train_masked, 2))\n",
    "        x_batch = gpu(X_train_masked[:, start_idx:end_idx])\n",
    "        y_batch = gpu(y_train_masked[:, start_idx:end_idx])\n",
    "\n",
    "        _, grads = Flux.withgradient(model) do m\n",
    "            loss(m, x_batch, y_batch, \"train\")\n",
    "        end\n",
    "        Flux.update!(opt, model, grads[1])\n",
    "        loss_val, logits_masked, y_masked = loss(model, x_batch, y_batch, \"test\")\n",
    "        \n",
    "        push!(epoch_train_losses, loss_val)\n",
    "\n",
    "        if !isempty(y_masked)\n",
    "            preds_masked = Flux.onecold(logits_masked) |> cpu\n",
    "            tp = sum(preds_masked .== cpu(y_masked))\n",
    "            total_masked = length(y_masked)\n",
    "            total_train_tp += tp\n",
    "            total_train_masked += total_masked\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # log metrics\n",
    "    epoch_train_loss = mean(epoch_train_losses)\n",
    "\n",
    "    epoch_train_acc = 0.0\n",
    "    if total_train_masked > 0\n",
    "        epoch_train_acc = total_train_tp / total_train_masked\n",
    "    end\n",
    "\n",
    "    push!(train_losses, epoch_train_loss)\n",
    "    push!(train_accuracies, epoch_train_acc)\n",
    "\n",
    "    \n",
    "    ### start test\n",
    "    epoch_test_losses = Float32[]\n",
    "    total_test_tp = 0\n",
    "    total_test_masked = 0\n",
    "\n",
    "    for start_idx in 1:batch_size:size(X_test_masked, 2)\n",
    "        end_idx = min(start_idx + batch_size - 1, size(X_test_masked, 2))\n",
    "        x_batch = gpu(X_test_masked[:, start_idx:end_idx])\n",
    "        y_batch = gpu(y_test_masked[:, start_idx:end_idx])\n",
    "\n",
    "        test_loss_val, logits_masked, y_masked = loss(model, x_batch, y_batch, \"test\")\n",
    "\n",
    "        push!(epoch_test_losses, test_loss_val)\n",
    "\n",
    "        if !isempty(y_masked)\n",
    "            preds_masked = Flux.onecold(logits_masked) |> cpu\n",
    "            tp = sum(preds_masked .== cpu(y_masked))\n",
    "            total_masked = length(y_masked)\n",
    "            total_test_tp += tp\n",
    "            total_test_masked += total_masked\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # log metrics\n",
    "    epoch_test_loss = mean(epoch_test_losses)\n",
    "\n",
    "    epoch_test_acc = 0.0\n",
    "    if total_test_masked > 0\n",
    "        epoch_test_acc = total_test_tp / total_test_masked\n",
    "    end\n",
    "\n",
    "    push!(test_losses, epoch_test_loss)\n",
    "    push!(test_accuracies, epoch_test_acc)\n",
    "\n",
    "    if epoch % 10 == 0\n",
    "        run_time = now() - start_time\n",
    "        total_minutes = div(run_time.value, 60000)\n",
    "        run_hours = div(total_minutes, 60)\n",
    "        run_minutes = rem(total_minutes, 60)\n",
    "\n",
    "        timestamp = Dates.format(now(), \"yyyy-mm-dd_HH-MM-SS\")\n",
    "        save_dir = joinpath(\"plots\", \"untrt\", \"indef_masked_new\", \"$(timestamp)_epoch_$(epoch)\")\n",
    "        mkpath(save_dir)\n",
    "\n",
    "        epochs_ran = 1:epoch\n",
    "\n",
    "        # loss plot\n",
    "        loss_plot = Plots.plot(epochs_ran, train_losses; label=\"train loss\",\n",
    "             xlabel=\"epoch\", ylabel=\"loss\", title=\"train vs. test loss (epoch $epoch)\", lw=2)\n",
    "        Plots.plot!(loss_plot, epochs_ran, test_losses; label=\"test loss\", lw=2)\n",
    "        savefig(joinpath(save_dir, \"trainval_loss.png\"))\n",
    "\n",
    "        # acc plot\n",
    "        acc_plot = Plots.plot(epochs_ran, train_accuracies; label=\"train accuracy\",\n",
    "            xlabel=\"epoch\", ylabel=\"accuracy\", title=\"train vs. test accuracy (epoch $epoch)\", lw=2, legend=:bottomright)\n",
    "        Plots.plot!(acc_plot, epochs_ran, test_accuracies; label=\"test accuracy\", lw=2)\n",
    "        savefig(joinpath(save_dir, \"accuracy.png\"))\n",
    "\n",
    "        # save acc to csv\n",
    "        df = DataFrame(\n",
    "            epoch = epochs_ran,\n",
    "            train_accuracy = train_accuracies,\n",
    "            test_accuracy = test_accuracies,\n",
    "            train_loss = train_losses,\n",
    "            test_loss = test_losses\n",
    "            )\n",
    "        CSV.write(joinpath(save_dir, \"metrics.csv\"), df)\n",
    "\n",
    "        # log params\n",
    "        params_txt = joinpath(save_dir, \"params.txt\")\n",
    "        open(params_txt, \"w\") do io\n",
    "            println(io, \"EPOCH @ SAVE: $epoch\")\n",
    "            println(io, \"TIMESTAMP: $timestamp\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12832e65",
   "metadata": {},
   "source": [
    "2. re-typing model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c67844b",
   "metadata": {},
   "source": [
    "previous code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7171e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, CUDA, JLD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80ce4719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can use GPU or CPU :D\n",
    "const IntMatrix2DType = Union{Array{Int}, CuArray{Int, 2}}\n",
    "const Float32Matrix2DType = Union{Array{Float32}, CuArray{Float32, 2}}\n",
    "const Float32Matrix3DType = Union{Array{Float32, 3}, CuArray{Float32, 3}}\n",
    "\n",
    "### positional encoder\n",
    "\n",
    "struct PosEnc\n",
    "    pe_matrix::CuArray{Float32,2}\n",
    "end\n",
    "\n",
    "function PosEnc(embed_dim::Int, max_len::Int) # max_len is usually maximum length of sequence but here it is just len(genes)\n",
    "    pe_matrix = Matrix{Float32}(undef, embed_dim, max_len)\n",
    "    for pos in 1:max_len, i in 1:embed_dim\n",
    "        angle = pos / (10000^(2*(div(i-1,2))/embed_dim))\n",
    "        if mod(i, 2) == 1\n",
    "            pe_matrix[i,pos] = sin(angle) # odd indices\n",
    "        else\n",
    "            pe_matrix[i,pos] = cos(angle) # even indices\n",
    "        end\n",
    "    end\n",
    "    return PosEnc(cu(pe_matrix))\n",
    "end\n",
    "\n",
    "Flux.@functor PosEnc\n",
    "\n",
    "function (pe::PosEnc)(input::Float32Matrix3DType)\n",
    "    seq_len = size(input,2)\n",
    "    return input .+ pe.pe_matrix[:,1:seq_len] # adds positional encoding to input embeddings\n",
    "end\n",
    "\n",
    "### building transformer section\n",
    "\n",
    "struct Transf\n",
    "    mha::Flux.MultiHeadAttention\n",
    "    att_dropout::Flux.Dropout\n",
    "    att_norm::Flux.LayerNorm # this is the normalization aspect\n",
    "    mlp::Flux.Chain\n",
    "    mlp_norm::Flux.LayerNorm\n",
    "end\n",
    "\n",
    "function Transf(\n",
    "    embed_dim::Int, \n",
    "    hidden_dim::Int; \n",
    "    n_heads::Int, \n",
    "    dropout_prob::Float64\n",
    "    )\n",
    "\n",
    "    mha = Flux.MultiHeadAttention((embed_dim, embed_dim, embed_dim) => (embed_dim, embed_dim) => embed_dim, \n",
    "                                    nheads=n_heads, \n",
    "                                    dropout_prob=dropout_prob\n",
    "                                    )\n",
    "\n",
    "    att_dropout = Flux.Dropout(dropout_prob)\n",
    "    \n",
    "    att_norm = Flux.LayerNorm(embed_dim)\n",
    "    \n",
    "    mlp = Flux.Chain(\n",
    "        Flux.Dense(embed_dim => hidden_dim, gelu),\n",
    "        Flux.Dropout(dropout_prob),\n",
    "        Flux.Dense(hidden_dim => embed_dim),\n",
    "        Flux.Dropout(dropout_prob)\n",
    "        )\n",
    "    mlp_norm = Flux.LayerNorm(embed_dim)\n",
    "\n",
    "    return Transf(mha, att_dropout, att_norm, mlp, mlp_norm)\n",
    "end\n",
    "\n",
    "Flux.@functor Transf\n",
    "\n",
    "function (tf::Transf)(input::Float32Matrix3DType) # input shape: embed_dim × seq_len × batch_size\n",
    "    normed = tf.att_norm(input)\n",
    "    atted = tf.mha(normed, normed, normed)[1] # outputs a tuple (a, b)\n",
    "    att_dropped = tf.att_dropout(atted)\n",
    "    residualed = input + att_dropped\n",
    "    res_normed = tf.mlp_norm(residualed)\n",
    "\n",
    "    embed_dim, seq_len, batch_size = size(res_normed)\n",
    "    reshaped = reshape(res_normed, embed_dim, seq_len * batch_size) # dense layers expect 2D inputs\n",
    "    mlp_out = tf.mlp(reshaped)\n",
    "    mlp_out_reshaped = reshape(mlp_out, embed_dim, seq_len, batch_size)\n",
    "    \n",
    "    tf_output = residualed + mlp_out_reshaped\n",
    "    return tf_output\n",
    "end\n",
    "\n",
    "### full model as << ranked data --> token embedding --> position embedding --> transformer --> classifier head >>\n",
    "\n",
    "struct Model\n",
    "    embedding::Flux.Embedding\n",
    "    pos_encoder::PosEnc\n",
    "    pos_dropout::Flux.Dropout\n",
    "    transformer::Flux.Chain\n",
    "    classifier::Flux.Chain\n",
    "end\n",
    "\n",
    "function Model(;\n",
    "    input_size::Int,\n",
    "    embed_dim::Int,\n",
    "    n_layers::Int,\n",
    "    n_classes::Int,\n",
    "    n_heads::Int,\n",
    "    hidden_dim::Int,\n",
    "    dropout_prob::Float64\n",
    "    )\n",
    "\n",
    "    embedding = Flux.Embedding(input_size => embed_dim)\n",
    "\n",
    "    pos_encoder = PosEnc(embed_dim, input_size)\n",
    "\n",
    "    pos_dropout = Flux.Dropout(dropout_prob)\n",
    "\n",
    "    transformer = Flux.Chain(\n",
    "        [Transf(embed_dim, hidden_dim; n_heads, dropout_prob) for _ in 1:n_layers]...\n",
    "        )\n",
    "\n",
    "    classifier = Flux.Chain(\n",
    "        Flux.Dense(embed_dim => embed_dim, gelu),\n",
    "        Flux.LayerNorm(embed_dim),\n",
    "        Flux.Dense(embed_dim => n_classes)\n",
    "        )\n",
    "\n",
    "    return Model(embedding, pos_encoder, pos_dropout, transformer, classifier)\n",
    "end\n",
    "\n",
    "Flux.@functor Model\n",
    "\n",
    "function (model::Model)(input::IntMatrix2DType)\n",
    "    embedded = model.embedding(input)\n",
    "    encoded = model.pos_encoder(embedded)\n",
    "    encoded_dropped = model.pos_dropout(encoded)\n",
    "    transformed = model.transformer(encoded_dropped)\n",
    "    # pooled = dropdims(mean(transformed; dims=2), dims=2)\n",
    "    logits_output = model.classifier(transformed)\n",
    "    return logits_output\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25afd9ae",
   "metadata": {},
   "source": [
    "new code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9693066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct PosEnc1{U<:AbstractMatrix} #!#\n",
    "    pe_matrix::U\n",
    "end\n",
    "\n",
    "function PosEnc1(embed_dim::Int, max_len::Int) # max_len is usually maximum length of sequence but here it is just len(genes)\n",
    "    pe_matrix = Matrix{Float32}(undef, embed_dim, max_len)\n",
    "    for pos in 1:max_len, i in 1:embed_dim\n",
    "        angle = pos / (10000^(2*(div(i-1,2))/embed_dim))\n",
    "        if mod(i, 2) == 1\n",
    "            pe_matrix[i,pos] = sin(angle) # odd indices\n",
    "        else\n",
    "            pe_matrix[i,pos] = cos(angle) # even indices\n",
    "        end\n",
    "    end\n",
    "    return PosEnc1(pe_matrix)\n",
    "end\n",
    "\n",
    "Flux.@functor PosEnc1\n",
    "\n",
    "function (pe::PosEnc1)(input::Float32Matrix3DType)\n",
    "    seq_len = size(input,2)\n",
    "    return input .+ pe.pe_matrix[:,1:seq_len] # adds positional encoding to input embeddings\n",
    "end\n",
    "\n",
    "### building transformer section\n",
    "\n",
    "struct Transf1{MHA<:Flux.MultiHeadAttention, D<:Flux.Dropout, LN<:Flux.LayerNorm, C<:Flux.Chain} #!#\n",
    "    mha::MHA\n",
    "    att_dropout::D\n",
    "    att_norm::LN\n",
    "    mlp::C\n",
    "    mlp_norm::LN\n",
    "end\n",
    "\n",
    "function Transf1(\n",
    "    embed_dim::Int, \n",
    "    hidden_dim::Int; \n",
    "    n_heads::Int, \n",
    "    dropout_prob::Float64\n",
    "    )\n",
    "\n",
    "    mha = Flux.MultiHeadAttention((embed_dim, embed_dim, embed_dim) => (embed_dim, embed_dim) => embed_dim, \n",
    "                                    nheads=n_heads, \n",
    "                                    dropout_prob=dropout_prob\n",
    "                                    )\n",
    "\n",
    "    att_dropout = Flux.Dropout(dropout_prob)\n",
    "    \n",
    "    att_norm = Flux.LayerNorm(embed_dim)\n",
    "    \n",
    "    mlp = Flux.Chain(\n",
    "        Flux.Dense(embed_dim => hidden_dim, gelu),\n",
    "        Flux.Dropout(dropout_prob),\n",
    "        Flux.Dense(hidden_dim => embed_dim),\n",
    "        Flux.Dropout(dropout_prob)\n",
    "        )\n",
    "    mlp_norm = Flux.LayerNorm(embed_dim)\n",
    "\n",
    "    return Transf(mha, att_dropout, att_norm, mlp, mlp_norm)\n",
    "end\n",
    "\n",
    "Flux.@functor Transf1\n",
    "\n",
    "function (tf::Transf1)(input::Float32Matrix3DType) # input shape: embed_dim × seq_len × batch_size\n",
    "    normed = tf.att_norm(input)\n",
    "    atted, _ = tf.mha(normed, normed, normed) # outputs a tuple (a, b)\n",
    "    att_dropped = tf.att_dropout(atted)\n",
    "    residualed = input + att_dropped\n",
    "    res_normed = tf.mlp_norm(residualed)\n",
    "\n",
    "    embed_dim, seq_len, batch_size = size(res_normed)\n",
    "    reshaped = reshape(res_normed, embed_dim, seq_len * batch_size) # dense layers expect 2D inputs\n",
    "    mlp_out = tf.mlp(reshaped)\n",
    "    mlp_out_reshaped = reshape(mlp_out, embed_dim, seq_len, batch_size)\n",
    "    \n",
    "    tf_output = residualed + mlp_out_reshaped\n",
    "    return tf_output\n",
    "end\n",
    "\n",
    "struct Model1{E<:Flux.Embedding, P<:PosEnc, D<:Flux.Dropout, T<:Flux.Chain, C<:Flux.Chain} #!#\n",
    "    embedding::E\n",
    "    pos_encoder::P\n",
    "    pos_dropout::D\n",
    "    transformer::T\n",
    "    classifier::C\n",
    "end\n",
    "\n",
    "function Model1(;\n",
    "    input_size::Int,\n",
    "    embed_dim::Int,\n",
    "    n_layers::Int,\n",
    "    n_classes::Int,\n",
    "    n_heads::Int,\n",
    "    hidden_dim::Int,\n",
    "    dropout_prob::Float64\n",
    "    )\n",
    "\n",
    "    embedding = Flux.Embedding(input_size => embed_dim)\n",
    "\n",
    "    pos_encoder = PosEnc(embed_dim, input_size)\n",
    "\n",
    "    pos_dropout = Flux.Dropout(dropout_prob)\n",
    "\n",
    "    transformer = Flux.Chain(\n",
    "    (Transf(embed_dim, hidden_dim; n_heads, dropout_prob) for _ in 1:n_layers)...\n",
    "    )\n",
    "\n",
    "    classifier = Flux.Chain(\n",
    "        Flux.Dense(embed_dim => embed_dim, gelu),\n",
    "        Flux.LayerNorm(embed_dim),\n",
    "        Flux.Dense(embed_dim => n_classes)\n",
    "        )\n",
    "\n",
    "    return Model1(embedding, pos_encoder, pos_dropout, transformer, classifier)\n",
    "end\n",
    "\n",
    "Flux.@functor Model1\n",
    "\n",
    "function (model::Model1)(input::T) where {T<:IntMatrix2DType} \n",
    "    embedded = model.embedding(input)\n",
    "    encoded = model.pos_encoder(embedded)\n",
    "    encoded_dropped = model.pos_dropout(encoded)\n",
    "    transformed = model.transformer(encoded_dropped)\n",
    "    logits_output = model.classifier(transformed)\n",
    "    return logits_output\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9602c0e",
   "metadata": {},
   "source": [
    "see testing.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35c70a",
   "metadata": {},
   "source": [
    "3. comparison of inputs => new graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb337d",
   "metadata": {},
   "source": [
    "scatter plot - binning vs. no binning\n",
    "\n",
    "heatmap - vs. box scatter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
